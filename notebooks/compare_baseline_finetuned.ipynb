{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeccc999",
   "metadata": {},
   "source": [
    "# Baseline vs Fine-tuned YOLOv11s Comparison\n",
    "\n",
    "## Overview\n",
    "\n",
    "Notebook nÃ y so sÃ¡nh performance giá»¯a:\n",
    "- **Baseline**: YOLOv11s pretrained (khÃ´ng fine-tune)\n",
    "- **Fine-tuned**: YOLOv11s Ä‘Ã£ fine-tune trÃªn chest X-ray data\n",
    "\n",
    "**CÃ¡c bÆ°á»›c thá»±c hiá»‡n:**\n",
    "1. Load cáº£ 2 models\n",
    "2. Evaluate trÃªn test set\n",
    "3. Táº¡o comparison plots:\n",
    "   - mAP comparison\n",
    "   - Precision/Recall comparison\n",
    "   - Per-class performance\n",
    "   - Confusion matrices\n",
    "4. LÆ°u plots cho bÃ¡o cÃ¡o\n",
    "\n",
    "**Prerequisites:**\n",
    "- Fine-tuned model Ä‘Ã£ Ä‘Æ°á»£c train (cháº¡y `train_yolov11s.ipynb`)\n",
    "- Preprocessed test data cÃ³ sáºµn\n",
    "\n",
    "**Thá»i gian hoÃ n thÃ nh:** ~15-20 phÃºt\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e468a1",
   "metadata": {},
   "source": [
    "## Section 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918c7452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory to repository root\n",
    "%cd /home/minhquana/workspace/project_DeepLearning/computer_vision/Abnormal-Prediction-In-Chest-X-Ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d62100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ“ Imports successful\")\n",
    "print(f\"  PyTorch version: {torch.__version__}\")\n",
    "print(f\"  CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019f00b3",
   "metadata": {},
   "source": [
    "## Section 2: Load Models and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22807b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model paths\n",
    "baseline_model_path = 'yolo11s.pt'  # Pretrained model\n",
    "finetuned_model_path = Path('backend/models/yolov11s_finetuned.pt')\n",
    "\n",
    "# Data configuration\n",
    "data_yaml_path = Path('data/preprocessed/data.yaml')\n",
    "\n",
    "print(\"Loading Models and Data Configuration\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check if fine-tuned model exists\n",
    "if not finetuned_model_path.exists():\n",
    "    print(f\"ERROR: Fine-tuned model not found at {finetuned_model_path}\")\n",
    "    print(\"\\nPlease run train_yolov11s.ipynb first to train the model.\")\n",
    "    raise FileNotFoundError(f\"Fine-tuned model not found\")\n",
    "\n",
    "# Check if data.yaml exists\n",
    "if not data_yaml_path.exists():\n",
    "    print(f\"ERROR: data.yaml not found at {data_yaml_path}\")\n",
    "    print(\"\\nPlease run data_preparation.ipynb first to create preprocessed data.\")\n",
    "    raise FileNotFoundError(f\"data.yaml not found\")\n",
    "\n",
    "print(f\"âœ“ Baseline model: {baseline_model_path}\")\n",
    "print(f\"âœ“ Fine-tuned model: {finetuned_model_path}\")\n",
    "print(f\"âœ“ Data config: {data_yaml_path}\")\n",
    "\n",
    "# Load data configuration\n",
    "with open(data_yaml_path, 'r') as f:\n",
    "    data_config = yaml.safe_load(f)\n",
    "\n",
    "class_names = data_config['names']\n",
    "num_classes = data_config['nc']\n",
    "\n",
    "print(f\"\\nDataset Info:\")\n",
    "print(f\"  Number of classes: {num_classes}\")\n",
    "print(f\"  Class names: {class_names}\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4497647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "print(\"\\nLoading models...\")\n",
    "\n",
    "print(\"  Loading baseline model...\")\n",
    "baseline_model = YOLO(baseline_model_path)\n",
    "print(\"    âœ“ Baseline model loaded\")\n",
    "\n",
    "print(\"  Loading fine-tuned model...\")\n",
    "finetuned_model = YOLO(str(finetuned_model_path))\n",
    "print(\"    âœ“ Fine-tuned model loaded\")\n",
    "\n",
    "print(\"\\nâœ“ Both models loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b1203a",
   "metadata": {},
   "source": [
    "## Section 3: Evaluate Models on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c4cd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nEvaluating Baseline Model on Test Set\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "baseline_metrics = baseline_model.val(\n",
    "    data=str(data_yaml_path),\n",
    "    split='test',\n",
    "    save_json=True,\n",
    "    save_hybrid=True,\n",
    "    conf=0.25,\n",
    "    iou=0.6,\n",
    "    max_det=300,\n",
    "    plots=True,\n",
    ")\n",
    "\n",
    "baseline_results = baseline_metrics.results_dict\n",
    "\n",
    "print(\"\\nBaseline Results:\")\n",
    "print(f\"  mAP50:       {baseline_results.get('metrics/mAP50(B)', 0):.4f}\")\n",
    "print(f\"  mAP50-95:    {baseline_results.get('metrics/mAP50-95(B)', 0):.4f}\")\n",
    "print(f\"  Precision:   {baseline_results.get('metrics/precision(B)', 0):.4f}\")\n",
    "print(f\"  Recall:      {baseline_results.get('metrics/recall(B)', 0):.4f}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251a2b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nEvaluating Fine-tuned Model on Test Set\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "finetuned_metrics = finetuned_model.val(\n",
    "    data=str(data_yaml_path),\n",
    "    split='test',\n",
    "    save_json=True,\n",
    "    save_hybrid=True,\n",
    "    conf=0.25,\n",
    "    iou=0.6,\n",
    "    max_det=300,\n",
    "    plots=True,\n",
    ")\n",
    "\n",
    "finetuned_results = finetuned_metrics.results_dict\n",
    "\n",
    "print(\"\\nFine-tuned Results:\")\n",
    "print(f\"  mAP50:       {finetuned_results.get('metrics/mAP50(B)', 0):.4f}\")\n",
    "print(f\"  mAP50-95:    {finetuned_results.get('metrics/mAP50-95(B)', 0):.4f}\")\n",
    "print(f\"  Precision:   {finetuned_results.get('metrics/precision(B)', 0):.4f}\")\n",
    "print(f\"  Recall:      {finetuned_results.get('metrics/recall(B)', 0):.4f}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5b11dc",
   "metadata": {},
   "source": [
    "## Section 4: Create Comparison Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d894d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plots directory\n",
    "plots_dir = Path('docs/comparison_plots')\n",
    "plots_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "print(f\"Plots will be saved to: {plots_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce290a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: Overall Metrics Comparison\n",
    "print(\"\\nCreating Overall Metrics Comparison...\")\n",
    "\n",
    "metrics_to_compare = [\n",
    "    ('mAP50', 'metrics/mAP50(B)'),\n",
    "    ('mAP50-95', 'metrics/mAP50-95(B)'),\n",
    "    ('Precision', 'metrics/precision(B)'),\n",
    "    ('Recall', 'metrics/recall(B)'),\n",
    "]\n",
    "\n",
    "baseline_values = [baseline_results.get(key, 0) for _, key in metrics_to_compare]\n",
    "finetuned_values = [finetuned_results.get(key, 0) for _, key in metrics_to_compare]\n",
    "metric_names = [name for name, _ in metrics_to_compare]\n",
    "\n",
    "x = np.arange(len(metric_names))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bars1 = ax.bar(x - width/2, baseline_values, width, label='Baseline', alpha=0.8, color='#FF6B6B')\n",
    "bars2 = ax.bar(x + width/2, finetuned_values, width, label='Fine-tuned', alpha=0.8, color='#4ECDC4')\n",
    "\n",
    "ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Baseline vs Fine-tuned YOLOv11s - Overall Metrics Comparison', fontsize=14, fontweight='bold', pad=20)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metric_names, fontsize=11)\n",
    "ax.legend(fontsize=11, loc='upper right')\n",
    "ax.set_ylim([0, 1.0])\n",
    "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Add value labels on bars\n",
    "def add_value_labels(bars):\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}',\n",
    "                ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "add_value_labels(bars1)\n",
    "add_value_labels(bars2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_path = plots_dir / f'overall_metrics_comparison_{timestamp}.png'\n",
    "plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"  âœ“ Saved to: {plot_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2984ee23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2: Improvement Percentage\n",
    "print(\"\\nCreating Improvement Percentage Plot...\")\n",
    "\n",
    "improvements = []\n",
    "for baseline_val, finetuned_val in zip(baseline_values, finetuned_values):\n",
    "    if baseline_val > 0:\n",
    "        improvement = ((finetuned_val - baseline_val) / baseline_val) * 100\n",
    "    else:\n",
    "        improvement = 0\n",
    "    improvements.append(improvement)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "colors = ['#2ECC71' if imp >= 0 else '#E74C3C' for imp in improvements]\n",
    "bars = ax.bar(metric_names, improvements, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "ax.set_ylabel('Improvement (%)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Performance Improvement: Fine-tuned vs Baseline', fontsize=14, fontweight='bold', pad=20)\n",
    "ax.axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
    "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Add value labels\n",
    "for bar, imp in zip(bars, improvements):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{imp:+.1f}%',\n",
    "            ha='center', va='bottom' if imp >= 0 else 'top',\n",
    "            fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_path = plots_dir / f'improvement_percentage_{timestamp}.png'\n",
    "plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"  âœ“ Saved to: {plot_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f715fe29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3: Per-Class mAP50 Comparison (if available)\n",
    "print(\"\\nCreating Per-Class mAP50 Comparison...\")\n",
    "\n",
    "# Try to get per-class AP from metrics\n",
    "if hasattr(baseline_metrics, 'box') and hasattr(baseline_metrics.box, 'ap_class_index'):\n",
    "    baseline_ap_per_class = baseline_metrics.box.ap50\n",
    "    finetuned_ap_per_class = finetuned_metrics.box.ap50\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Class': class_names[:len(baseline_ap_per_class)],\n",
    "        'Baseline': baseline_ap_per_class,\n",
    "        'Fine-tuned': finetuned_ap_per_class,\n",
    "    })\n",
    "    \n",
    "    # Sort by improvement\n",
    "    comparison_df['Improvement'] = comparison_df['Fine-tuned'] - comparison_df['Baseline']\n",
    "    comparison_df = comparison_df.sort_values('Improvement', ascending=False)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, max(8, len(comparison_df) * 0.5)))\n",
    "    \n",
    "    y_pos = np.arange(len(comparison_df))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax.barh(y_pos - width/2, comparison_df['Baseline'], width, label='Baseline', alpha=0.8, color='#FF6B6B')\n",
    "    bars2 = ax.barh(y_pos + width/2, comparison_df['Fine-tuned'], width, label='Fine-tuned', alpha=0.8, color='#4ECDC4')\n",
    "    \n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(comparison_df['Class'], fontsize=10)\n",
    "    ax.set_xlabel('mAP50', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Per-Class mAP50 Comparison', fontsize=14, fontweight='bold', pad=20)\n",
    "    ax.legend(fontsize=11, loc='lower right')\n",
    "    ax.set_xlim([0, 1.0])\n",
    "    ax.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plot_path = plots_dir / f'per_class_map50_{timestamp}.png'\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"  âœ“ Saved to: {plot_path}\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Display table\n",
    "    print(\"\\nðŸ“‹ Per-Class mAP50 Table:\")\n",
    "    print(comparison_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"  Per-class AP not available in metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4331e718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 4: Model Size and Speed Comparison\n",
    "print(\"\\nâš¡ Creating Model Size and Speed Comparison...\")\n",
    "\n",
    "# Get model info\n",
    "baseline_size = Path(baseline_model_path).stat().st_size / (1024 * 1024)  # MB\n",
    "finetuned_size = finetuned_model_path.stat().st_size / (1024 * 1024)  # MB\n",
    "\n",
    "# Get inference speed (if available from metrics)\n",
    "baseline_speed = baseline_results.get('speed/inference', 0) if 'speed/inference' in baseline_results else 0\n",
    "finetuned_speed = finetuned_results.get('speed/inference', 0) if 'speed/inference' in finetuned_results else 0\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Model size comparison\n",
    "models = ['Baseline', 'Fine-tuned']\n",
    "sizes = [baseline_size, finetuned_size]\n",
    "colors = ['#FF6B6B', '#4ECDC4']\n",
    "\n",
    "bars1 = ax1.bar(models, sizes, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "ax1.set_ylabel('Model Size (MB)', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Model Size Comparison', fontsize=13, fontweight='bold')\n",
    "ax1.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "for bar, size in zip(bars1, sizes):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{size:.1f} MB',\n",
    "             ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Inference speed comparison (if available)\n",
    "if baseline_speed > 0 and finetuned_speed > 0:\n",
    "    speeds = [baseline_speed, finetuned_speed]\n",
    "    bars2 = ax2.bar(models, speeds, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "    ax2.set_ylabel('Inference Speed (ms)', fontsize=12, fontweight='bold')\n",
    "    ax2.set_title('Inference Speed Comparison', fontsize=13, fontweight='bold')\n",
    "    ax2.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "    \n",
    "    for bar, speed in zip(bars2, speeds):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                 f'{speed:.2f} ms',\n",
    "                 ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "else:\n",
    "    ax2.text(0.5, 0.5, 'Speed data not available', \n",
    "             ha='center', va='center', fontsize=12, transform=ax2.transAxes)\n",
    "    ax2.set_title('Inference Speed Comparison', fontsize=13, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_path = plots_dir / f'model_size_speed_{timestamp}.png'\n",
    "plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"  âœ“ Saved to: {plot_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e171edad",
   "metadata": {},
   "source": [
    "## Section 5: Generate Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b22f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary report\n",
    "print(\"\\nGenerating Summary Report...\")\n",
    "\n",
    "report_lines = [\n",
    "    \"# Baseline vs Fine-tuned YOLOv11s - Comparison Report\\n\",\n",
    "    f\"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\",\n",
    "    \"## Models\\n\",\n",
    "    f\"- **Baseline**: YOLOv11s pretrained (`{baseline_model_path}`)\\n\",\n",
    "    f\"- **Fine-tuned**: YOLOv11s fine-tuned on chest X-ray data (`{finetuned_model_path}`)\\n\\n\",\n",
    "    \"## Dataset\\n\",\n",
    "    f\"- **Test set**: `{data_yaml_path}`\\n\",\n",
    "    f\"- **Number of classes**: {num_classes}\\n\",\n",
    "    f\"- **Classes**: {', '.join(class_names)}\\n\\n\",\n",
    "    \"## Overall Performance Comparison\\n\\n\",\n",
    "    \"| Metric | Baseline | Fine-tuned | Improvement |\\n\",\n",
    "    \"|--------|----------|------------|-------------|\\n\",\n",
    "]\n",
    "\n",
    "for (name, _), baseline_val, finetuned_val, improvement in zip(\n",
    "    metrics_to_compare, baseline_values, finetuned_values, improvements\n",
    "):\n",
    "    report_lines.append(\n",
    "        f\"| {name} | {baseline_val:.4f} | {finetuned_val:.4f} | {improvement:+.2f}% |\\n\"\n",
    "    )\n",
    "\n",
    "report_lines.extend([\n",
    "    \"\\n## Key Findings\\n\\n\",\n",
    "])\n",
    "\n",
    "# Add key findings based on improvements\n",
    "avg_improvement = np.mean(improvements)\n",
    "if avg_improvement > 0:\n",
    "    report_lines.append(f\"**Overall improvement**: {avg_improvement:.2f}% average increase across metrics\\n\\n\")\n",
    "else:\n",
    "    report_lines.append(f\"**Overall performance**: {avg_improvement:.2f}% change (needs investigation)\\n\\n\")\n",
    "\n",
    "# Find best and worst performing metrics\n",
    "best_metric_idx = np.argmax(improvements)\n",
    "worst_metric_idx = np.argmin(improvements)\n",
    "\n",
    "report_lines.extend([\n",
    "    f\"- **Best improvement**: {metric_names[best_metric_idx]} ({improvements[best_metric_idx]:+.2f}%)\\n\",\n",
    "    f\"- **Least improvement**: {metric_names[worst_metric_idx]} ({improvements[worst_metric_idx]:+.2f}%)\\n\\n\",\n",
    "])\n",
    "\n",
    "report_lines.extend([\n",
    "    \"## Model Properties\\n\\n\",\n",
    "    f\"- **Baseline size**: {baseline_size:.2f} MB\\n\",\n",
    "    f\"- **Fine-tuned size**: {finetuned_size:.2f} MB\\n\",\n",
    "])\n",
    "\n",
    "if baseline_speed > 0 and finetuned_speed > 0:\n",
    "    report_lines.extend([\n",
    "        f\"- **Baseline inference**: {baseline_speed:.2f} ms\\n\",\n",
    "        f\"- **Fine-tuned inference**: {finetuned_speed:.2f} ms\\n\",\n",
    "    ])\n",
    "\n",
    "report_lines.extend([\n",
    "    \"\\n## Visualizations\\n\\n\",\n",
    "    f\"All comparison plots saved to: `{plots_dir}/`\\n\\n\",\n",
    "    \"- Overall metrics comparison\\n\",\n",
    "    \"- Improvement percentage\\n\",\n",
    "    \"- Per-class mAP50 (if available)\\n\",\n",
    "    \"- Model size and speed\\n\\n\",\n",
    "    \"## Conclusion\\n\\n\",\n",
    "])\n",
    "\n",
    "if avg_improvement > 5:\n",
    "    report_lines.append(\n",
    "        \"The fine-tuned model shows **significant improvement** over the baseline, \"\n",
    "        \"indicating successful domain adaptation to chest X-ray abnormality detection. \"\n",
    "        \"The model is ready for production deployment.\\n\"\n",
    "    )\n",
    "elif avg_improvement > 0:\n",
    "    report_lines.append(\n",
    "        \"The fine-tuned model shows **modest improvement** over the baseline. \"\n",
    "        \"Consider additional training epochs or data augmentation strategies.\\n\"\n",
    "    )\n",
    "else:\n",
    "    report_lines.append(\n",
    "        \"The fine-tuned model shows **no improvement** or degradation. \"\n",
    "        \"Review training configuration, data quality, and augmentation strategies.\\n\"\n",
    "    )\n",
    "\n",
    "# Save report\n",
    "report_path = plots_dir / f'comparison_report_{timestamp}.md'\n",
    "with open(report_path, 'w', encoding='utf-8') as f:\n",
    "    f.writelines(report_lines)\n",
    "\n",
    "print(f\"  âœ“ Report saved to: {report_path}\")\n",
    "\n",
    "# Display report\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPARISON REPORT\")\n",
    "print(\"=\" * 80)\n",
    "print(''.join(report_lines))\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fe0854",
   "metadata": {},
   "source": [
    "## Section 6: Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6ef49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCOMPARISON COMPLETED\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nGenerated Assets:\")\n",
    "print(f\"  Plots directory: {plots_dir}/\")\n",
    "print(f\"  Summary report: {report_path}\")\n",
    "\n",
    "print(\"\\nQuick Summary:\")\n",
    "print(f\"  Average improvement: {avg_improvement:+.2f}%\")\n",
    "print(f\"  Best metric: {metric_names[best_metric_idx]} ({improvements[best_metric_idx]:+.2f}%)\")\n",
    "print(f\"  Baseline mAP50: {baseline_results.get('metrics/mAP50(B)', 0):.4f}\")\n",
    "print(f\"  Fine-tuned mAP50: {finetuned_results.get('metrics/mAP50(B)', 0):.4f}\")\n",
    "\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"  1. Review generated plots in docs/comparison_plots/\")\n",
    "print(\"  2. Include plots in your report/presentation\")\n",
    "print(\"  3. Analyze per-class performance for weak classes\")\n",
    "print(\"  4. Deploy fine-tuned model if performance is satisfactory\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
