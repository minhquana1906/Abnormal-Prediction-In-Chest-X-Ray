{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eda43cf3",
   "metadata": {},
   "source": [
    "# Data Preparation - Chest X-ray Abnormality Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1282a20",
   "metadata": {},
   "source": [
    "## Section 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d0cdfb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/minhquana/workspace/project_DeepLearning/computer_vision/Abnormal-Prediction-In-Chest-X-Ray\n"
     ]
    }
   ],
   "source": [
    "# Set working directory to repository root\n",
    "%cd /home/minhquana/workspace/project_DeepLearning/computer_vision/Abnormal-Prediction-In-Chest-X-Ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb36998a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Roboflow if not already installed\n",
    "# !pip install -q roboflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1083043b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Imports successful\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Tuple\n",
    "import yaml\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from roboflow import Roboflow\n",
    "\n",
    "# Import preprocessing utilities\n",
    "import sys\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "\n",
    "from backend.src.utils.preprocessing import preprocess_image\n",
    "# from backend.src.utils.class_mapping import get_vietnamese_class_name\n",
    "\n",
    "print(\"‚úì Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699681aa",
   "metadata": {},
   "source": [
    "## Section 2: Download Dataset from Roboflow\n",
    "\n",
    "Download VinBigData Chest X-ray Symptom Detection dataset version 3 (YOLOv11 format)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b016780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Dataset from Roboflow\n",
      "================================================================================\n",
      "  Workspace: vinbigdataxrayproject\n",
      "  Project: chest-xray-symptom-detection\n",
      "  Version: 3\n",
      "  Format: yolov11\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Roboflow configuration\n",
    "ROBOFLOW_API_KEY = \"wQ9S049DhK8xjIhNy6zv\"\n",
    "WORKSPACE_NAME = \"vinbigdataxrayproject\"\n",
    "PROJECT_NAME = \"chest-xray-symptom-detection\"\n",
    "VERSION = 3\n",
    "FORMAT = \"yolov11\"\n",
    "\n",
    "print(\"Downloading Dataset from Roboflow\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"  Workspace: {WORKSPACE_NAME}\")\n",
    "print(f\"  Project: {PROJECT_NAME}\")\n",
    "print(f\"  Version: {VERSION}\")\n",
    "print(f\"  Format: {FORMAT}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "340d1d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in data/ to yolov11:: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1110489/1110489 [01:43<00:00, 10774.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to data/ in yolov11:: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30008/30008 [00:02<00:00, 13078.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Dataset downloaded to: /home/minhquana/workspace/project_DeepLearning/computer_vision/Abnormal-Prediction-In-Chest-X-Ray/data\n"
     ]
    }
   ],
   "source": [
    "# Download dataset\n",
    "rf = Roboflow(api_key=ROBOFLOW_API_KEY)\n",
    "project = rf.workspace(WORKSPACE_NAME).project(PROJECT_NAME)\n",
    "version = project.version(VERSION)\n",
    "\n",
    "# Download to data/ directory\n",
    "dataset = version.download(FORMAT, location=\"data/\", overwrite=True)\n",
    "\n",
    "print(f\"\\n‚úì Dataset downloaded to: {dataset.location}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5de0f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Information:\n",
      "  Classes: 14\n",
      "  Class names: ['Aortic enlargement', 'Atelectasis', 'Calcification', 'Cardiomegaly', 'Consolidation', 'ILD', 'Infiltration', 'Lung Opacity', 'Nodule-Mass', 'Other lesion', 'Pleural effusion', 'Pleural thickening', 'Pneumothorax', 'Pulmonary fibrosis']\n",
      "  Train: 10,499 images\n",
      "  Valid: 3,000 images\n",
      "  Test: 1,499 images\n"
     ]
    }
   ],
   "source": [
    "# Verify downloaded data\n",
    "dataset_dir = Path(dataset.location)\n",
    "data_yaml_path = dataset_dir / \"data.yaml\"\n",
    "\n",
    "if not data_yaml_path.exists():\n",
    "    raise FileNotFoundError(f\"data.yaml not found at {data_yaml_path}\")\n",
    "\n",
    "# Load data.yaml to get class information\n",
    "with open(data_yaml_path, 'r') as f:\n",
    "    data_config = yaml.safe_load(f)\n",
    "\n",
    "print(\"\\nDataset Information:\")\n",
    "print(f\"  Classes: {data_config['nc']}\")\n",
    "print(f\"  Class names: {data_config['names']}\")\n",
    "\n",
    "# Count images in each split\n",
    "for split in ['train', 'valid', 'test']:\n",
    "    split_dir = dataset_dir / split / 'images'\n",
    "    if split_dir.exists():\n",
    "        count = len(list(split_dir.glob('*.jpg'))) + len(list(split_dir.glob('*.png')))\n",
    "        print(f\"  {split.capitalize()}: {count:,} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91d6265",
   "metadata": {},
   "source": [
    "## Section 3: Class Mapping to Vietnamese\n",
    "\n",
    "Map English class names to Vietnamese for better display in the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc35de97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Mapping (English -> Vietnamese):\n",
      "================================================================================\n",
      "  Aortic enlargement             -> Ph√¨nh ƒë·ªông m·∫°ch ch·ªß\n",
      "  Atelectasis                    -> X·∫πp ph·ªïi\n",
      "  Calcification                  -> V√¥i h√≥a\n",
      "  Cardiomegaly                   -> Tim to\n",
      "  Consolidation                  -> ƒê√¥ng ƒë·∫∑c ph·ªïi\n",
      "  ILD                            -> B·ªánh ph·ªïi k·∫Ω\n",
      "  Infiltration                   -> Th√¢m nhi·ªÖm\n",
      "  Lung Opacity                   -> ƒê·ª•c ph·ªïi\n",
      "  Nodule/Mass                    -> N·ªët/Kh·ªëi u\n",
      "  Other lesion                   -> T·ªïn th∆∞∆°ng kh√°c\n",
      "  Pleural effusion               -> Tr√†n d·ªãch m√†ng ph·ªïi\n",
      "  Pleural thickening             -> D√†y m√†ng ph·ªïi\n",
      "  Pneumothorax                   -> Tr√†n kh√≠ m√†ng ph·ªïi\n",
      "  Pulmonary fibrosis             -> X∆° ph·ªïi\n",
      "  Normal                         -> B√¨nh th∆∞·ªùng\n",
      "================================================================================\n",
      "\n",
      "‚úì Class mapping saved to: configs/class_mapping_vi.json\n"
     ]
    }
   ],
   "source": [
    "# Class mapping English -> Vietnamese\n",
    "CLASS_MAPPING_VI = {\n",
    "    \"Aortic enlargement\": \"Ph√¨nh ƒë·ªông m·∫°ch ch·ªß\",\n",
    "    \"Atelectasis\": \"X·∫πp ph·ªïi\",\n",
    "    \"Calcification\": \"V√¥i h√≥a\",\n",
    "    \"Cardiomegaly\": \"Tim to\",\n",
    "    \"Consolidation\": \"ƒê√¥ng ƒë·∫∑c ph·ªïi\",\n",
    "    \"ILD\": \"B·ªánh ph·ªïi k·∫Ω\",\n",
    "    \"Infiltration\": \"Th√¢m nhi·ªÖm\",\n",
    "    \"Lung Opacity\": \"ƒê·ª•c ph·ªïi\",\n",
    "    \"Nodule/Mass\": \"N·ªët/Kh·ªëi u\",\n",
    "    \"Other lesion\": \"T·ªïn th∆∞∆°ng kh√°c\",\n",
    "    \"Pleural effusion\": \"Tr√†n d·ªãch m√†ng ph·ªïi\",\n",
    "    \"Pleural thickening\": \"D√†y m√†ng ph·ªïi\",\n",
    "    \"Pneumothorax\": \"Tr√†n kh√≠ m√†ng ph·ªïi\",\n",
    "    \"Pulmonary fibrosis\": \"X∆° ph·ªïi\",\n",
    "    \"Normal\": \"B√¨nh th∆∞·ªùng\",\n",
    "}\n",
    "\n",
    "print(\"Class Mapping (English -> Vietnamese):\")\n",
    "print(\"=\" * 80)\n",
    "for eng, vie in CLASS_MAPPING_VI.items():\n",
    "    print(f\"  {eng:30s} -> {vie}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Save mapping to configs/\n",
    "config_dir = Path('configs')\n",
    "config_dir.mkdir(exist_ok=True)\n",
    "\n",
    "mapping_file = config_dir / 'class_mapping_vi.json'\n",
    "with open(mapping_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(CLASS_MAPPING_VI, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\n‚úì Class mapping saved to: {mapping_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84ebb99",
   "metadata": {},
   "source": [
    "## Section 4: Analyze Dataset and Label \"Normal\" Images\n",
    "\n",
    "- Identify images without bounding boxes (no abnormalities)\n",
    "- Label them as \"Normal\" class\n",
    "- Count samples per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6231dcd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10499/10499 [00:00<00:00, 52271.10it/s]\n",
      "Analyzing valid: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3000/3000 [00:00<00:00, 64892.82it/s]\n",
      "Analyzing test: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1499/1499 [00:00<00:00, 65956.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Analysis:\n",
      "================================================================================\n",
      "\n",
      "TRAIN:\n",
      "  Total images: 10,499\n",
      "  Abnormal (with labels): 3,051\n",
      "  Normal (no labels): 7,448 (70.9%)\n",
      "\n",
      "VALID:\n",
      "  Total images: 3,000\n",
      "  Abnormal (with labels): 919\n",
      "  Normal (no labels): 2,081 (69.4%)\n",
      "\n",
      "TEST:\n",
      "  Total images: 1,499\n",
      "  Abnormal (with labels): 424\n",
      "  Normal (no labels): 1,075 (71.7%)\n",
      "\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def analyze_dataset(dataset_dir: Path) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze dataset and identify images without labels (Normal cases).\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with analysis results\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'train': {'images': [], 'labels': [], 'normal_images': []},\n",
    "        'valid': {'images': [], 'labels': [], 'normal_images': []},\n",
    "        'test': {'images': [], 'labels': [], 'normal_images': []},\n",
    "    }\n",
    "    \n",
    "    class_counts = {split: Counter() for split in ['train', 'valid', 'test']}\n",
    "    \n",
    "    for split in ['train', 'valid', 'test']:\n",
    "        images_dir = dataset_dir / split / 'images'\n",
    "        labels_dir = dataset_dir / split / 'labels'\n",
    "        \n",
    "        if not images_dir.exists():\n",
    "            continue\n",
    "        \n",
    "        image_files = sorted(images_dir.glob('*.jpg')) + sorted(images_dir.glob('*.png'))\n",
    "        \n",
    "        for img_path in tqdm(image_files, desc=f\"Analyzing {split}\"):\n",
    "            results[split]['images'].append(img_path)\n",
    "            \n",
    "            # Check if label file exists\n",
    "            label_path = labels_dir / (img_path.stem + '.txt')\n",
    "            \n",
    "            if label_path.exists() and label_path.stat().st_size > 0:\n",
    "                # Has labels - count classes\n",
    "                results[split]['labels'].append(label_path)\n",
    "                \n",
    "                with open(label_path, 'r') as f:\n",
    "                    for line in f:\n",
    "                        parts = line.strip().split()\n",
    "                        if len(parts) >= 5:\n",
    "                            class_id = int(parts[0])\n",
    "                            class_counts[split][class_id] += 1\n",
    "            else:\n",
    "                # No labels - Normal case\n",
    "                results[split]['normal_images'].append(img_path)\n",
    "    \n",
    "    return results, class_counts\n",
    "\n",
    "print(\"Analyzing dataset...\")\n",
    "analysis_results, class_counts = analyze_dataset(dataset_dir)\n",
    "\n",
    "print(\"\\nDataset Analysis:\")\n",
    "print(\"=\" * 80)\n",
    "for split in ['train', 'valid', 'test']:\n",
    "    total_images = len(analysis_results[split]['images'])\n",
    "    normal_images = len(analysis_results[split]['normal_images'])\n",
    "    abnormal_images = total_images - normal_images\n",
    "    \n",
    "    print(f\"\\n{split.upper()}:\")\n",
    "    print(f\"  Total images: {total_images:,}\")\n",
    "    print(f\"  Abnormal (with labels): {abnormal_images:,}\")\n",
    "    print(f\"  Normal (no labels): {normal_images:,} ({normal_images/total_images*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4a3850d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class Distribution (Abnormal Cases):\n",
      "================================================================================\n",
      "\n",
      "TRAIN:\n",
      "  [ 0] Aortic enlargement             (Ph√¨nh ƒë·ªông m·∫°ch ch·ªß           ): 2,134 (20.04%)\n",
      "  [ 1] Atelectasis                    (X·∫πp ph·ªïi                      ):   131 ( 1.23%)\n",
      "  [ 2] Calcification                  (V√¥i h√≥a                       ):   304 ( 2.85%)\n",
      "  [ 3] Cardiomegaly                   (Tim to                        ): 1,590 (14.93%)\n",
      "  [ 4] Consolidation                  (ƒê√¥ng ƒë·∫∑c ph·ªïi                 ):   241 ( 2.26%)\n",
      "  [ 5] ILD                            (B·ªánh ph·ªïi k·∫Ω                  ):   246 ( 2.31%)\n",
      "  [ 6] Infiltration                   (Th√¢m nhi·ªÖm                    ):   434 ( 4.08%)\n",
      "  [ 7] Lung Opacity                   (ƒê·ª•c ph·ªïi                      ):   909 ( 8.54%)\n",
      "  [ 8] Nodule-Mass                    (Nodule-Mass                   ):   577 ( 5.42%)\n",
      "  [ 9] Other lesion                   (T·ªïn th∆∞∆°ng kh√°c               ):   796 ( 7.48%)\n",
      "  [10] Pleural effusion               (Tr√†n d·ªãch m√†ng ph·ªïi           ):   706 ( 6.63%)\n",
      "  [11] Pleural thickening             (D√†y m√†ng ph·ªïi                 ): 1,378 (12.94%)\n",
      "  [12] Pneumothorax                   (Tr√†n kh√≠ m√†ng ph·ªïi            ):    70 ( 0.66%)\n",
      "  [13] Pulmonary fibrosis             (X∆° ph·ªïi                       ): 1,132 (10.63%)\n",
      "\n",
      "VALID:\n",
      "  [ 0] Aortic enlargement             (Ph√¨nh ƒë·ªông m·∫°ch ch·ªß           ):   632 (19.92%)\n",
      "  [ 1] Atelectasis                    (X·∫πp ph·ªïi                      ):    35 ( 1.10%)\n",
      "  [ 2] Calcification                  (V√¥i h√≥a                       ):   106 ( 3.34%)\n",
      "  [ 3] Cardiomegaly                   (Tim to                        ):   492 (15.51%)\n",
      "  [ 4] Consolidation                  (ƒê√¥ng ƒë·∫∑c ph·ªïi                 ):    64 ( 2.02%)\n",
      "  [ 5] ILD                            (B·ªánh ph·ªïi k·∫Ω                  ):   100 ( 3.15%)\n",
      "  [ 6] Infiltration                   (Th√¢m nhi·ªÖm                    ):   121 ( 3.81%)\n",
      "  [ 7] Lung Opacity                   (ƒê·ª•c ph·ªïi                      ):   276 ( 8.70%)\n",
      "  [ 8] Nodule-Mass                    (Nodule-Mass                   ):   180 ( 5.67%)\n",
      "  [ 9] Other lesion                   (T·ªïn th∆∞∆°ng kh√°c               ):   220 ( 6.94%)\n",
      "  [10] Pleural effusion               (Tr√†n d·ªãch m√†ng ph·ªïi           ):   209 ( 6.59%)\n",
      "  [11] Pleural thickening             (D√†y m√†ng ph·ªïi                 ):   397 (12.52%)\n",
      "  [12] Pneumothorax                   (Tr√†n kh√≠ m√†ng ph·ªïi            ):    21 ( 0.66%)\n",
      "  [13] Pulmonary fibrosis             (X∆° ph·ªïi                       ):   319 (10.06%)\n",
      "\n",
      "TEST:\n",
      "  [ 0] Aortic enlargement             (Ph√¨nh ƒë·ªông m·∫°ch ch·ªß           ):   301 (19.48%)\n",
      "  [ 1] Atelectasis                    (X·∫πp ph·ªïi                      ):    20 ( 1.29%)\n",
      "  [ 2] Calcification                  (V√¥i h√≥a                       ):    42 ( 2.72%)\n",
      "  [ 3] Cardiomegaly                   (Tim to                        ):   218 (14.11%)\n",
      "  [ 4] Consolidation                  (ƒê√¥ng ƒë·∫∑c ph·ªïi                 ):    48 ( 3.11%)\n",
      "  [ 5] ILD                            (B·ªánh ph·ªïi k·∫Ω                  ):    40 ( 2.59%)\n",
      "  [ 6] Infiltration                   (Th√¢m nhi·ªÖm                    ):    58 ( 3.75%)\n",
      "  [ 7] Lung Opacity                   (ƒê·ª•c ph·ªïi                      ):   137 ( 8.87%)\n",
      "  [ 8] Nodule-Mass                    (Nodule-Mass                   ):    69 ( 4.47%)\n",
      "  [ 9] Other lesion                   (T·ªïn th∆∞∆°ng kh√°c               ):   118 ( 7.64%)\n",
      "  [10] Pleural effusion               (Tr√†n d·ªãch m√†ng ph·ªïi           ):   117 ( 7.57%)\n",
      "  [11] Pleural thickening             (D√†y m√†ng ph·ªïi                 ):   206 (13.33%)\n",
      "  [12] Pneumothorax                   (Tr√†n kh√≠ m√†ng ph·ªïi            ):     5 ( 0.32%)\n",
      "  [13] Pulmonary fibrosis             (X∆° ph·ªïi                       ):   166 (10.74%)\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Display class distribution\n",
    "print(\"\\nClass Distribution (Abnormal Cases):\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "class_names = data_config['names']\n",
    "\n",
    "for split in ['train', 'valid', 'test']:\n",
    "    print(f\"\\n{split.upper()}:\")\n",
    "    total_annotations = sum(class_counts[split].values())\n",
    "    \n",
    "    for class_id in sorted(class_counts[split].keys()):\n",
    "        count = class_counts[split][class_id]\n",
    "        class_name = class_names[class_id] if class_id < len(class_names) else f\"Class_{class_id}\"\n",
    "        class_name_vi = CLASS_MAPPING_VI.get(class_name, class_name)\n",
    "        \n",
    "        print(f\"  [{class_id:2d}] {class_name:30s} ({class_name_vi:30s}): {count:5,} ({count/total_annotations*100:5.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958c2f97",
   "metadata": {},
   "source": [
    "## Section 5: Sample \"Normal\" Images (30% ~ 2000 images)\n",
    "\n",
    "Keep only 30% of \"Normal\" images to balance the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "952d81c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling 30% of Normal images...\n",
      "================================================================================\n",
      "  TRAIN:\n",
      "    Original Normal images: 7,448\n",
      "    Sampled Normal images: 2,000 (26.9%)\n",
      "  VALID:\n",
      "    Original Normal images: 2,081\n",
      "    Sampled Normal images: 624 (30.0%)\n",
      "  TEST:\n",
      "    Original Normal images: 1,075\n",
      "    Sampled Normal images: 322 (30.0%)\n",
      "================================================================================\n",
      "\n",
      "‚úì Total sampled Normal images: 2,946\n"
     ]
    }
   ],
   "source": [
    "def sample_normal_images(\n",
    "    normal_images: List[Path],\n",
    "    target_ratio: float = 0.3,\n",
    "    max_count: int = 2000,\n",
    "    random_seed: int = 42\n",
    ") -> List[Path]:\n",
    "    random.seed(random_seed)\n",
    "    \n",
    "    target_count = min(int(len(normal_images) * target_ratio), max_count)\n",
    "    sampled = random.sample(normal_images, target_count)\n",
    "    \n",
    "    return sampled\n",
    "\n",
    "# Sample normal images for each split\n",
    "sampled_normal = {}\n",
    "\n",
    "print(\"Sampling 30% of Normal images...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for split in ['train', 'valid', 'test']:\n",
    "    normal_images = analysis_results[split]['normal_images']\n",
    "    \n",
    "    if len(normal_images) == 0:\n",
    "        sampled_normal[split] = []\n",
    "        continue\n",
    "    \n",
    "    sampled = sample_normal_images(normal_images, target_ratio=0.3, max_count=2000)\n",
    "    sampled_normal[split] = sampled\n",
    "    \n",
    "    print(f\"  {split.upper()}:\")\n",
    "    print(f\"    Original Normal images: {len(normal_images):,}\")\n",
    "    print(f\"    Sampled Normal images: {len(sampled):,} ({len(sampled)/len(normal_images)*100:.1f}%)\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n‚úì Total sampled Normal images: {sum(len(sampled_normal[s]) for s in ['train', 'valid', 'test']):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861ecb6a",
   "metadata": {},
   "source": [
    "## Section 6: Remove Classes with Low Sample Count\n",
    "\n",
    "Remove classes that have fewer than threshold samples (default 700)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "048d0d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtering Classes (min_samples=1000)\n",
      "================================================================================\n",
      "\n",
      "KEPT CLASSES:\n",
      "  [ 0] Aortic enlargement             (Ph√¨nh ƒë·ªông m·∫°ch ch·ªß           ): 3,067 samples\n",
      "  [ 3] Cardiomegaly                   (Tim to                        ): 2,300 samples\n",
      "  [ 7] Lung Opacity                   (ƒê·ª•c ph·ªïi                      ): 1,322 samples\n",
      "  [ 9] Other lesion                   (T·ªïn th∆∞∆°ng kh√°c               ): 1,134 samples\n",
      "  [10] Pleural effusion               (Tr√†n d·ªãch m√†ng ph·ªïi           ): 1,032 samples\n",
      "  [11] Pleural thickening             (D√†y m√†ng ph·ªïi                 ): 1,981 samples\n",
      "  [13] Pulmonary fibrosis             (X∆° ph·ªïi                       ): 1,617 samples\n",
      "\n",
      "REMOVED CLASSES:\n",
      "  [ 1] Atelectasis                    (X·∫πp ph·ªïi                      ):   186 samples (< 1000)\n",
      "  [ 2] Calcification                  (V√¥i h√≥a                       ):   452 samples (< 1000)\n",
      "  [ 4] Consolidation                  (ƒê√¥ng ƒë·∫∑c ph·ªïi                 ):   353 samples (< 1000)\n",
      "  [ 5] ILD                            (B·ªánh ph·ªïi k·∫Ω                  ):   386 samples (< 1000)\n",
      "  [ 6] Infiltration                   (Th√¢m nhi·ªÖm                    ):   613 samples (< 1000)\n",
      "  [ 8] Nodule-Mass                    (Nodule-Mass                   ):   826 samples (< 1000)\n",
      "  [12] Pneumothorax                   (Tr√†n kh√≠ m√†ng ph·ªïi            ):    96 samples (< 1000)\n",
      "================================================================================\n",
      "\n",
      "‚úì Kept 7 classes, removed 7 classes\n"
     ]
    }
   ],
   "source": [
    "def filter_classes_by_count(\n",
    "    class_counts: Dict[str, Counter],\n",
    "    min_samples: int = 700\n",
    ") -> Tuple[List[int], List[int]]:\n",
    "    \"\"\"\n",
    "    Filter classes based on minimum sample count.\n",
    "    \n",
    "    Args:\n",
    "        class_counts: Dictionary of class counts per split\n",
    "        min_samples: Minimum number of samples required\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (kept_class_ids, removed_class_ids)\n",
    "    \"\"\"\n",
    "    # Count total samples per class across all splits\n",
    "    total_counts = Counter()\n",
    "    for split_counts in class_counts.values():\n",
    "        total_counts.update(split_counts)\n",
    "    \n",
    "    # Filter classes\n",
    "    kept_classes = [class_id for class_id, count in total_counts.items() if count >= min_samples]\n",
    "    removed_classes = [class_id for class_id, count in total_counts.items() if count < min_samples]\n",
    "    \n",
    "    return sorted(kept_classes), sorted(removed_classes)\n",
    "\n",
    "# Filter classes with < 1000 samples\n",
    "MIN_SAMPLES = 1000\n",
    "\n",
    "kept_classes, removed_classes = filter_classes_by_count(class_counts, min_samples=MIN_SAMPLES)\n",
    "\n",
    "print(f\"\\nFiltering Classes (min_samples={MIN_SAMPLES})\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Count total samples per class\n",
    "total_class_counts = Counter()\n",
    "for split_counts in class_counts.values():\n",
    "    total_class_counts.update(split_counts)\n",
    "\n",
    "print(\"\\nKEPT CLASSES:\")\n",
    "for class_id in kept_classes:\n",
    "    class_name = class_names[class_id] if class_id < len(class_names) else f\"Class_{class_id}\"\n",
    "    class_name_vi = CLASS_MAPPING_VI.get(class_name, class_name)\n",
    "    count = total_class_counts[class_id]\n",
    "    print(f\"  [{class_id:2d}] {class_name:30s} ({class_name_vi:30s}): {count:5,} samples\")\n",
    "\n",
    "print(\"\\nREMOVED CLASSES:\")\n",
    "for class_id in removed_classes:\n",
    "    class_name = class_names[class_id] if class_id < len(class_names) else f\"Class_{class_id}\"\n",
    "    class_name_vi = CLASS_MAPPING_VI.get(class_name, class_name)\n",
    "    count = total_class_counts[class_id]\n",
    "    print(f\"  [{class_id:2d}] {class_name:30s} ({class_name_vi:30s}): {count:5,} samples (< {MIN_SAMPLES})\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n‚úì Kept {len(kept_classes)} classes, removed {len(removed_classes)} classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43b613c",
   "metadata": {},
   "source": [
    "## Section 7: Create Preprocessed Dataset\n",
    "\n",
    "Apply preprocessing pipeline to all images:\n",
    "1. Grayscale conversion (if needed)\n",
    "2. Histogram equalization\n",
    "3. Normalization to [0, 1]\n",
    "\n",
    "Save preprocessed images and updated labels to `data/preprocessed/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13bd903c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating Preprocessed Dataset\n",
      "================================================================================\n",
      "\n",
      "Processing TRAIN split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Abnormal images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3051/3051 [08:42<00:00,  5.84it/s]\n",
      "  Normal images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2000/2000 [05:43<00:00,  5.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚úì Processed 3,019 abnormal images\n",
      "    ‚úì Processed 2,000 normal images\n",
      "    ‚ö† Skipped 32 images\n",
      "\n",
      "Processing VALID split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Abnormal images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 919/919 [02:38<00:00,  5.80it/s]\n",
      "  Normal images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 624/624 [01:45<00:00,  5.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚úì Processed 906 abnormal images\n",
      "    ‚úì Processed 624 normal images\n",
      "    ‚ö† Skipped 13 images\n",
      "\n",
      "Processing TEST split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Abnormal images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 424/424 [01:15<00:00,  5.59it/s]\n",
      "  Normal images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 322/322 [00:57<00:00,  5.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚úì Processed 423 abnormal images\n",
      "    ‚úì Processed 322 normal images\n",
      "    ‚ö† Skipped 1 images\n",
      "\n",
      "================================================================================\n",
      "‚úì Preprocessed dataset created successfully!\n",
      "  Output directory: /home/minhquana/workspace/project_DeepLearning/computer_vision/Abnormal-Prediction-In-Chest-X-Ray/data/preprocessed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def create_preprocessed_dataset(\n",
    "    dataset_dir: Path,\n",
    "    output_dir: Path,\n",
    "    analysis_results: Dict,\n",
    "    sampled_normal: Dict,\n",
    "    kept_classes: List[int],\n",
    "    class_names: List[str],\n",
    "    class_mapping_vi: Dict[str, str],\n",
    "):\n",
    "    \"\"\"\n",
    "    Create preprocessed dataset with filtered images and labels.\n",
    "    \n",
    "    - Apply preprocessing to all images\n",
    "    - Keep only selected normal images\n",
    "    - Filter out removed classes\n",
    "    - Update class IDs to be sequential\n",
    "    - Create \"Normal\" class (class_id = len(kept_classes))\n",
    "    \"\"\"\n",
    "    # Create output directory structure\n",
    "    for split in ['train', 'valid', 'test']:\n",
    "        (output_dir / split / 'images').mkdir(parents=True, exist_ok=True)\n",
    "        (output_dir / split / 'labels').mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Create class ID mapping (old -> new)\n",
    "    old_to_new_id = {old_id: new_id for new_id, old_id in enumerate(kept_classes)}\n",
    "    \n",
    "    # Add \"Normal\" class as last class\n",
    "    normal_class_id = len(kept_classes)\n",
    "    \n",
    "    # Process each split\n",
    "    stats = {'train': {}, 'valid': {}, 'test': {}}\n",
    "    \n",
    "    for split in ['train', 'valid', 'test']:\n",
    "        print(f\"\\nProcessing {split.upper()} split...\")\n",
    "        \n",
    "        processed_count = 0\n",
    "        skipped_count = 0\n",
    "        normal_count = 0\n",
    "        \n",
    "        # Process abnormal images (with labels)\n",
    "        images_with_labels = [\n",
    "            img for img in analysis_results[split]['images']\n",
    "            if img not in analysis_results[split]['normal_images']\n",
    "        ]\n",
    "        \n",
    "        for img_path in tqdm(images_with_labels, desc=f\"  Abnormal images\"):\n",
    "            label_path = dataset_dir / split / 'labels' / (img_path.stem + '.txt')\n",
    "            \n",
    "            if not label_path.exists():\n",
    "                continue\n",
    "            \n",
    "            # Read and filter labels\n",
    "            filtered_labels = []\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) >= 5:\n",
    "                        old_class_id = int(parts[0])\n",
    "                        \n",
    "                        # Keep only classes in kept_classes\n",
    "                        if old_class_id in old_to_new_id:\n",
    "                            new_class_id = old_to_new_id[old_class_id]\n",
    "                            filtered_labels.append(f\"{new_class_id} {' '.join(parts[1:])}\")\n",
    "            \n",
    "            # Skip images with no valid labels after filtering\n",
    "            if len(filtered_labels) == 0:\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "            \n",
    "            # Preprocess image\n",
    "            try:\n",
    "                img = Image.open(img_path).convert('L')\n",
    "                img_array = np.array(img)\n",
    "                preprocessed = preprocess_image(img_array, apply_normalization=False)\n",
    "                \n",
    "                # Convert back to uint8 for saving\n",
    "                preprocessed_uint8 = (preprocessed).astype(np.uint8)\n",
    "                \n",
    "                # Save preprocessed image\n",
    "                output_img_path = output_dir / split / 'images' / img_path.name\n",
    "                Image.fromarray(preprocessed_uint8).save(output_img_path)\n",
    "                \n",
    "                # Save filtered labels\n",
    "                output_label_path = output_dir / split / 'labels' / (img_path.stem + '.txt')\n",
    "                with open(output_label_path, 'w') as f:\n",
    "                    f.write('\\n'.join(filtered_labels))\n",
    "                \n",
    "                processed_count += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    Error processing {img_path.name}: {e}\")\n",
    "                skipped_count += 1\n",
    "        \n",
    "        # Process normal images (sampled)\n",
    "        for img_path in tqdm(sampled_normal[split], desc=f\"  Normal images\"):\n",
    "            try:\n",
    "                img = Image.open(img_path).convert('L')\n",
    "                img_array = np.array(img)\n",
    "                preprocessed = preprocess_image(img_array, apply_normalization=False)\n",
    "                \n",
    "                # Convert back to uint8 for saving\n",
    "                preprocessed_uint8 = (preprocessed).astype(np.uint8)\n",
    "                \n",
    "                # Save preprocessed image\n",
    "                output_img_path = output_dir / split / 'images' / img_path.name\n",
    "                Image.fromarray(preprocessed_uint8).save(output_img_path)\n",
    "                \n",
    "                # Create empty label file (no bounding box for normal)\n",
    "                output_label_path = output_dir / split / 'labels' / (img_path.stem + '.txt')\n",
    "                output_label_path.touch()\n",
    "                \n",
    "                normal_count += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    Error processing {img_path.name}: {e}\")\n",
    "        \n",
    "        stats[split] = {\n",
    "            'abnormal': processed_count,\n",
    "            'normal': normal_count,\n",
    "            'skipped': skipped_count,\n",
    "            'total': processed_count + normal_count\n",
    "        }\n",
    "        \n",
    "        print(f\"    ‚úì Processed {processed_count:,} abnormal images\")\n",
    "        print(f\"    ‚úì Processed {normal_count:,} normal images\")\n",
    "        print(f\"    ‚ö† Skipped {skipped_count:,} images\")\n",
    "    \n",
    "    # Create updated data.yaml\n",
    "    new_class_names = [class_names[old_id] for old_id in kept_classes] + [\"Normal\"]\n",
    "    new_class_names_vi = [class_mapping_vi.get(name, name) for name in new_class_names]\n",
    "    \n",
    "    data_yaml = {\n",
    "        'path': str(output_dir.absolute()),\n",
    "        'train': 'train/images',\n",
    "        'val': 'valid/images',\n",
    "        'test': 'test/images',\n",
    "        'nc': len(new_class_names),\n",
    "        'names': new_class_names,\n",
    "    }\n",
    "    \n",
    "    with open(output_dir / 'data.yaml', 'w') as f:\n",
    "        yaml.dump(data_yaml, f, default_flow_style=False, sort_keys=False)\n",
    "    \n",
    "    # Create data_vi.yaml with Vietnamese class names\n",
    "    data_yaml_vi = data_yaml.copy()\n",
    "    data_yaml_vi['names'] = new_class_names_vi\n",
    "    \n",
    "    with open(output_dir / 'data_vi.yaml', 'w', encoding='utf-8') as f:\n",
    "        yaml.dump(data_yaml_vi, f, default_flow_style=False, sort_keys=False, allow_unicode=True)\n",
    "    \n",
    "    return stats, new_class_names, new_class_names_vi\n",
    "\n",
    "# Create preprocessed dataset\n",
    "output_dir = Path('data/preprocessed')\n",
    "\n",
    "print(\"\\nCreating Preprocessed Dataset\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "stats, new_class_names, new_class_names_vi = create_preprocessed_dataset(\n",
    "    dataset_dir=dataset_dir,\n",
    "    output_dir=output_dir,\n",
    "    analysis_results=analysis_results,\n",
    "    sampled_normal=sampled_normal,\n",
    "    kept_classes=kept_classes,\n",
    "    class_names=class_names,\n",
    "    class_mapping_vi=CLASS_MAPPING_VI,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úì Preprocessed dataset created successfully!\")\n",
    "print(f\"  Output directory: {output_dir.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4810fd2",
   "metadata": {},
   "source": [
    "## Section 8: Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9e62fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FINAL DATASET SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Dataset Location:\n",
      "  /home/minhquana/workspace/project_DeepLearning/computer_vision/Abnormal-Prediction-In-Chest-X-Ray/data/preprocessed\n",
      "\n",
      " Classes (English):\n",
      "  [ 0] Aortic enlargement\n",
      "  [ 1] Cardiomegaly\n",
      "  [ 2] Lung Opacity\n",
      "  [ 3] Other lesion\n",
      "  [ 4] Pleural effusion\n",
      "  [ 5] Pleural thickening\n",
      "  [ 6] Pulmonary fibrosis\n",
      "  [ 7] Normal\n",
      "\n",
      " Classes (Ti·∫øng Vi·ªát):\n",
      "  [ 0] Ph√¨nh ƒë·ªông m·∫°ch ch·ªß\n",
      "  [ 1] Tim to\n",
      "  [ 2] ƒê·ª•c ph·ªïi\n",
      "  [ 3] T·ªïn th∆∞∆°ng kh√°c\n",
      "  [ 4] Tr√†n d·ªãch m√†ng ph·ªïi\n",
      "  [ 5] D√†y m√†ng ph·ªïi\n",
      "  [ 6] X∆° ph·ªïi\n",
      "  [ 7] B√¨nh th∆∞·ªùng\n",
      "\n",
      "Images per Split:\n",
      "\n",
      "  TRAIN:\n",
      "    Total: 5,019\n",
      "    Abnormal: 3,019 (60.2%)\n",
      "    Normal: 2,000 (39.8%)\n",
      "    Skipped: 32\n",
      "\n",
      "  VALID:\n",
      "    Total: 1,530\n",
      "    Abnormal: 906 (59.2%)\n",
      "    Normal: 624 (40.8%)\n",
      "    Skipped: 13\n",
      "\n",
      "  TEST:\n",
      "    Total: 745\n",
      "    Abnormal: 423 (56.8%)\n",
      "    Normal: 322 (43.2%)\n",
      "    Skipped: 1\n",
      "\n",
      "================================================================================\n",
      "\n",
      "TOTAL: 7,294 images\n",
      "   Abnormal: 4,348 (59.6%)\n",
      "   Normal: 2,946 (40.4%)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Display final statistics\n",
    "print(\"\\nFINAL DATASET SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nDataset Location:\")\n",
    "print(f\"  {output_dir.absolute()}\")\n",
    "\n",
    "print(\"\\n Classes (English):\")\n",
    "for i, name in enumerate(new_class_names):\n",
    "    print(f\"  [{i:2d}] {name}\")\n",
    "\n",
    "print(\"\\n Classes (Ti·∫øng Vi·ªát):\")\n",
    "for i, name in enumerate(new_class_names_vi):\n",
    "    print(f\"  [{i:2d}] {name}\")\n",
    "\n",
    "print(\"\\nImages per Split:\")\n",
    "for split in ['train', 'valid', 'test']:\n",
    "    print(f\"\\n  {split.upper()}:\")\n",
    "    print(f\"    Total: {stats[split]['total']:,}\")\n",
    "    print(f\"    Abnormal: {stats[split]['abnormal']:,} ({stats[split]['abnormal']/stats[split]['total']*100:.1f}%)\")\n",
    "    print(f\"    Normal: {stats[split]['normal']:,} ({stats[split]['normal']/stats[split]['total']*100:.1f}%)\")\n",
    "    print(f\"    Skipped: {stats[split]['skipped']:,}\")\n",
    "\n",
    "total_images = sum(stats[s]['total'] for s in ['train', 'valid', 'test'])\n",
    "total_abnormal = sum(stats[s]['abnormal'] for s in ['train', 'valid', 'test'])\n",
    "total_normal = sum(stats[s]['normal'] for s in ['train', 'valid', 'test'])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"\\nTOTAL: {total_images:,} images\")\n",
    "print(f\"   Abnormal: {total_abnormal:,} ({total_abnormal/total_images*100:.1f}%)\")\n",
    "print(f\"   Normal: {total_normal:,} ({total_normal/total_images*100:.1f}%)\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09425ac",
   "metadata": {},
   "source": [
    "## Section 9: (Optional) Create Gaussian Blur Augmented Data\n",
    "\n",
    "T·∫°o th√™m augmented versions c·ªßa training images v·ªõi Gaussian blur.\n",
    "- Ch·ªâ augment **training set** (kh√¥ng augment valid/test)\n",
    "- M·ªói ·∫£nh t·∫°o th√™m 1 augmented version\n",
    "- L∆∞u v√†o folder m·ªõi: `data/preprocessed_with_aug/`\n",
    "- C·∫≠p nh·∫≠t data.yaml t∆∞∆°ng ·ª©ng\n",
    "\n",
    "**L∆∞u √Ω:** Section n√†y l√† OPTIONAL. Ch·∫°y n·∫øu mu·ªën th√™m Gaussian blur augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b6d83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üé® Creating Augmented Dataset\n",
      "================================================================================\n",
      "  Source: data/preprocessed\n",
      "  Output: data/preprocessed_with_aug\n",
      "  Augment train only: True\n",
      "  Augmentations per image: 1\n",
      "================================================================================\n",
      "\n",
      "üîÑ Processing TRAIN split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Copying originals: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5019/5019 [00:00<00:00, 8474.65it/s]\n",
      "  Creating augmented versions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5019/5019 [02:11<00:00, 38.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚úì Original: 5,019\n",
      "    ‚úì Augmented: 5,019\n",
      "    ‚úì Total: 10,038\n",
      "\n",
      "üîÑ Processing VALID split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Copying originals: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1530/1530 [00:00<00:00, 8390.74it/s]\n",
      "  Copying originals: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1530/1530 [00:00<00:00, 8390.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚úì Original: 1,530\n",
      "    ‚úì Augmented: 0\n",
      "    ‚úì Total: 1,530\n",
      "\n",
      "üîÑ Processing TEST split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Copying originals: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 745/745 [00:00<00:00, 8002.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚úì Original: 745\n",
      "    ‚úì Augmented: 0\n",
      "    ‚úì Total: 745\n",
      "\n",
      "================================================================================\n",
      "‚úì Augmented dataset created successfully!\n",
      "  Output directory: /home/minhquana/workspace/project_DeepLearning/computer_vision/Abnormal-Prediction-In-Chest-X-Ray/data/preprocessed_with_aug\n",
      "================================================================================\n",
      "\n",
      "üìä Augmented Dataset Summary:\n",
      "================================================================================\n",
      "\n",
      "TRAIN:\n",
      "  Original images: 5,019\n",
      "  Augmented images: 5,019\n",
      "  Total images: 10,038\n",
      "\n",
      "VALID:\n",
      "  Original images: 1,530\n",
      "  Augmented images: 0\n",
      "  Total images: 1,530\n",
      "\n",
      "TEST:\n",
      "  Original images: 745\n",
      "  Augmented images: 0\n",
      "  Total images: 745\n",
      "\n",
      "================================================================================\n",
      "\n",
      "GRAND TOTAL:\n",
      "  Original: 7,294\n",
      "  Augmented: 5,019\n",
      "  Total: 12,313\n",
      "  Augmentation ratio: 68.8%\n",
      "================================================================================\n",
      "\n",
      "üí° To use augmented data for training:\n",
      "   Update data_yaml in train_yolov11s.ipynb to:\n",
      "   data_yaml = Path('data/preprocessed_with_aug/data.yaml')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def create_augmented_dataset(\n",
    "    source_dir: Path,\n",
    "    output_dir: Path,\n",
    "    augment_train_only: bool = True,\n",
    "    num_augmentations: int = 1,\n",
    "):\n",
    "    from backend.src.utils.augmentation import augment_image\n",
    "    \n",
    "    print(f\"\\nüé® Creating Augmented Dataset\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"  Source: {source_dir}\")\n",
    "    print(f\"  Output: {output_dir}\")\n",
    "    print(f\"  Augment train only: {augment_train_only}\")\n",
    "    print(f\"  Augmentations per image: {num_augmentations}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Determine which splits to process\n",
    "    splits_to_augment = ['train'] if augment_train_only else ['train', 'valid', 'test']\n",
    "    all_splits = ['train', 'valid', 'test']\n",
    "    \n",
    "    aug_stats = {}\n",
    "    \n",
    "    for split in all_splits:\n",
    "        source_images_dir = source_dir / split / 'images'\n",
    "        source_labels_dir = source_dir / split / 'labels'\n",
    "        \n",
    "        output_images_dir = output_dir / split / 'images'\n",
    "        output_labels_dir = output_dir / split / 'labels'\n",
    "        \n",
    "        output_images_dir.mkdir(parents=True, exist_ok=True)\n",
    "        output_labels_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        if not source_images_dir.exists():\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nProcessing {split.upper()} split...\")\n",
    "        \n",
    "        # Get all images\n",
    "        image_files = list(source_images_dir.glob('*.jpg')) + list(source_images_dir.glob('*.png'))\n",
    "        \n",
    "        original_count = 0\n",
    "        augmented_count = 0\n",
    "        \n",
    "        # Copy original images\n",
    "        for img_path in tqdm(image_files, desc=f\"  Copying originals\"):\n",
    "            # Copy image\n",
    "            shutil.copy(img_path, output_images_dir / img_path.name)\n",
    "            \n",
    "            # Copy label\n",
    "            label_path = source_labels_dir / (img_path.stem + '.txt')\n",
    "            if label_path.exists():\n",
    "                shutil.copy(label_path, output_labels_dir / label_path.name)\n",
    "            \n",
    "            original_count += 1\n",
    "        \n",
    "        # Create augmented versions (only for specified splits)\n",
    "        if split in splits_to_augment:\n",
    "            for img_path in tqdm(image_files, desc=f\"  Creating augmented versions\"):\n",
    "                # Load image\n",
    "                img = Image.open(img_path).convert('L')\n",
    "                img_array = np.array(img)\n",
    "                \n",
    "                # Create N augmented versions\n",
    "                for aug_idx in range(num_augmentations):\n",
    "                    # Apply Gaussian blur augmentation\n",
    "                    img_augmented = augment_image(img_array, augmentation_probability=1.0)\n",
    "                    \n",
    "                    # Save augmented image with suffix\n",
    "                    aug_img_name = f\"{img_path.stem}_aug{aug_idx+1}{img_path.suffix}\"\n",
    "                    aug_img_path = output_images_dir / aug_img_name\n",
    "                    Image.fromarray(img_augmented).save(aug_img_path)\n",
    "                    \n",
    "                    # Copy label with same suffix\n",
    "                    label_path = source_labels_dir / (img_path.stem + '.txt')\n",
    "                    if label_path.exists():\n",
    "                        aug_label_name = f\"{img_path.stem}_aug{aug_idx+1}.txt\"\n",
    "                        aug_label_path = output_labels_dir / aug_label_name\n",
    "                        shutil.copy(label_path, aug_label_path)\n",
    "                    \n",
    "                    augmented_count += 1\n",
    "        \n",
    "        total_count = original_count + augmented_count\n",
    "        aug_stats[split] = {\n",
    "            'original': original_count,\n",
    "            'augmented': augmented_count,\n",
    "            'total': total_count\n",
    "        }\n",
    "        \n",
    "        print(f\"    ‚úì Original: {original_count:,}\")\n",
    "        print(f\"    ‚úì Augmented: {augmented_count:,}\")\n",
    "        print(f\"    ‚úì Total: {total_count:,}\")\n",
    "    \n",
    "    # Copy and update data.yaml\n",
    "    source_yaml = source_dir / 'data.yaml'\n",
    "    output_yaml = output_dir / 'data.yaml'\n",
    "    \n",
    "    with open(source_yaml, 'r') as f:\n",
    "        data_yaml = yaml.safe_load(f)\n",
    "    \n",
    "    # Update path\n",
    "    data_yaml['path'] = str(output_dir.absolute())\n",
    "    \n",
    "    with open(output_yaml, 'w') as f:\n",
    "        yaml.dump(data_yaml, f, default_flow_style=False, sort_keys=False)\n",
    "    \n",
    "    # Copy data_vi.yaml if exists\n",
    "    source_yaml_vi = source_dir / 'data_vi.yaml'\n",
    "    if source_yaml_vi.exists():\n",
    "        output_yaml_vi = output_dir / 'data_vi.yaml'\n",
    "        with open(source_yaml_vi, 'r', encoding='utf-8') as f:\n",
    "            data_yaml_vi = yaml.safe_load(f)\n",
    "        data_yaml_vi['path'] = str(output_dir.absolute())\n",
    "        with open(output_yaml_vi, 'w', encoding='utf-8') as f:\n",
    "            yaml.dump(data_yaml_vi, f, default_flow_style=False, sort_keys=False, allow_unicode=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"‚úì Augmented dataset created successfully!\")\n",
    "    print(f\"  Output directory: {output_dir.absolute()}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return aug_stats\n",
    "\n",
    "\n",
    "augmented_output_dir = Path('data/preprocessed_with_aug')\n",
    "\n",
    "aug_stats = create_augmented_dataset(\n",
    "    source_dir=output_dir,\n",
    "    output_dir=augmented_output_dir,\n",
    "    augment_train_only=True,\n",
    "    num_augmentations=1,\n",
    ")\n",
    "\n",
    "print(\"\\nAugmented Dataset Summary:\")\n",
    "print(\"=\" * 80)\n",
    "for split in ['train', 'valid', 'test']:\n",
    "    print(f\"\\n{split.upper()}:\")\n",
    "    print(f\"  Original images: {aug_stats[split]['original']:,}\")\n",
    "    print(f\"  Augmented images: {aug_stats[split]['augmented']:,}\")\n",
    "    print(f\"  Total images: {aug_stats[split]['total']:,}\")\n",
    "\n",
    "total_original = sum(aug_stats[s]['original'] for s in ['train', 'valid', 'test'])\n",
    "total_augmented = sum(aug_stats[s]['augmented'] for s in ['train', 'valid', 'test'])\n",
    "total_all = sum(aug_stats[s]['total'] for s in ['train', 'valid', 'test'])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"\\nGRAND TOTAL:\")\n",
    "print(f\"  Original: {total_original:,}\")\n",
    "print(f\"  Augmented: {total_augmented:,}\")\n",
    "print(f\"  Total: {total_all:,}\")\n",
    "print(f\"  Augmentation ratio: {total_augmented/total_original*100:.1f}%\")\n",
    "print(\"=\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Abnormal-Prediction-In-Chest-X-Ray",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
